---
title: "Useful functions for biometrical models"
output: rmarkdown::html_vignette
link-citations: true
bibliography: METAABref.bib 
vignette: >
  %\VignetteIndexEntry{Indexes for simultaneous selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Find possible outliers in a data frame
The function `find_outlier()` may be used to identify possible outliers in a dataframe. It is suggested that before applying any statistical procedures, outliers be checked.

```{r, message=FALSE, fig.height = 5, fig.width = 5.5, fig.align = "center" }

library(METAAB)
library(dplyr)
library(cowplot)
data_out = data_ge2
data_out[12, 4] = 26
find_outliers(data_out, var = PH, plots = TRUE)
    ```

To check the outliers in different levels of a factor, the function `split_factors()` is used. As an example, we will find possible outliers for each level of the factor `ENV`.


```{r, message=FALSE, fig.height = 5, fig.width = 5.5, fig.align = "center" }
data_ge2 %>%
split_factors(ENV) %>%
find_outliers(PH)
```
    
# Correlations
## Linear and partial correlation coefficients

Pearson's linear correlation does not consider the influence a set of traits on the relationship between two traits. For example, the hypothetical correlation of *r* = 0.9 between *x* and *y* may be due to the influence of a third trait or group of traits acting together. To identify this linear effect between *x* and *y* controlling statistically the effect of others traits, the partial correlation is used. From Pearson's simple correlation matrix, the partial correlation is calculated by the following equation:

$$
	{r_{xy.m}} = \frac{{ - {a_{xy}}}}{{\sqrt {{a_{xx}}{a_{yy}}} }}
$$

Where ${r_{xy.m}}$ is the partial correlation coefficient between the traits * x * and * y *, excluding the effects of the * m * remaining traits of the set; $- {a_{ij}}$ is the inverse element of the correlation matrix corresponding to xy, ${a_{ii}}{a_{jj}}$ are the diagonal elements of the inverse matrix of correlation associated with trait x and y , respectively. The significance of this correlation is also tested by the test * t * according to the following expression:


$$
t_{calc} = r_{xy.m} \sqrt \frac{n-v}{1-r_{xy.m}^2} 
$$

Where $t_{calc}$ is the calculated Student * t * statistic; $ r_{xy.m} $ is the partial correlation coefficient for the traits x and y excluding the effect of the other * m * traits; * n * is the number of observations; and * v * is the number of traits. Both the linear and partial correlation coefficients may be obtained using the function `lpcor()`.


```{r, message=FALSE }
dataset = data_ge2

lpc1 = lpcor(dataset[,5:8])

# Compute the correlations for each level of the factor ENV
lpc2 = dataset %>%
       split_factors(ENV) %>%
       lpcor(verbose = FALSE) # Don't show the result in the console

    ```


Using the `pairs_mantel()` function, it is possible to compute a Mantel's test [@Mantel1967] for all pairwise correlation matrices of the above example. 

```{r, fig.height = 5, fig.width = 5.5, fig.align = "center" }
lpc2 %>% pairs_mantel(names = paste("H", 1:4, sep = ""))


    ```

This same plot may be obtained by passing correlation matrices with the same dimension to an object of class `lpcor` and then applying the function `pairs_mantel()`.

```{r, fig.height = 5, fig.width = 5.5, fig.align = "center" }
as.lpcor(cor(dataset[1:30, 5:ncol(dataset)]),
         cor(dataset[31:60, 5:ncol(dataset)]),
         cor(dataset[61:90, 5:ncol(dataset)]),
         cor(dataset[91:120, 5:ncol(dataset)]),
         cor(dataset[121:150, 5:ncol(dataset)])) %>%
  pairs_mantel(diag = TRUE,
               pan.spacing = 0,
               shape.point = 21,
               col.point = "black",
               fill.point = "red",
               size.point = 1.5,
               alpha.point = 0.6,
               main = "My own plot",
               alpha = 0.2)
    ```




## Graphical and numerical visualization of a correlation matrix

The function `corr_plot()` may be used to visualize (both graphically and numerically) a correlation matrix. Pairwise of scatterplots are produced and may be shown in the upper or lower diagonal, which may be seen as a nicer and customizable ggplot2-based version of the `pairs()` function.

```{r, fig.height = 5, fig.width = 5.5, fig.align = "center" }
dataset = data_ge2

corr_plot(dataset, CD, EL, PERK, NKR)


corr_plot(dataset, CD, EL, PERK, NKR,
          lower = NULL,
          upper = "corr")

corr_plot(dataset, CD, EL, PERK, NKR,
          shape.point = 19,
          size.point = 2,
          alpha.point = 0.5,
          alpha.diag = 0,
          pan.spacing = 0,
          col.sign = "gray",
          alpha.sign = 0.3,
          axis.labels = TRUE)


corr_plot(dataset, CD, EL, PERK, NKR, CW, NKE,
          prob = 0.01,
          shape.point = 21,
          col.point = "black",
          fill.point = "orange",
          size.point = 2,
          alpha.point = 0.6,
          maxsize = 4,
          minsize = 2,
          smooth = TRUE,
          size.smooth = 1,
          col.smooth = "black",
          col.sign = "cyan",
          col.up.panel = "black",
          col.lw.panel = "black",
          col.dia.panel = "black",
          pan.spacing = 0,
          lab.position = "tl")

```


## (co)variance and correlations for designed experiments

The function `covcor_design()` may be used to compute genetic, phenotypic and residual correlation/(co)variance matrices through Analysis of Variance (ANOVA) method using randomized complete block design (RCBD) or completely randomized design (CRD).

The phenotypic ($r_p$), genotypic ($r_g$) and residual ($r_r$) correlations are computed as follows:

$$
r^p_{xy} = \frac{cov^p_{xy}}{\sqrt{var^p_{x}var^p_{y}}} \
r^g_{xy} = \frac{cov^g_{xy}}{\sqrt{var^g_{x}var^g_{y}}} \
r^r_{xy} = \frac{cov^r_{xy}}{\sqrt{var^r_{x}var^r_{y}}}
$$

Using Mean Squares (*MS*) from the ANOVA method, the variances (*var*) and covariances (*cov*) are computed as follows:

$$
cov^p_{xy} = [(MST_{x+y} - MST_x - MST_y)/2]/r \\
var^p_x = MST_x / r \\
var^p_y = MST_y / r
$$


$$
cov^r_{xy} = (MSR_{x+y} - MSR_x - MSR_y)/2 \\
var^r_x = MSR_x \\
var^r_y = MSR_y 
$$

$$
cov^g_{xy} = [(cov^p_{xy} \times r) - cov^r_{xy}]/r \\
var^g_x = (MST_x - MSE_x)/r \\
var^g_y = (MST_x - MSE_y)/r 
$$

where *MST* is the mean square for treatment, *MSR* is the mean square for residuals, and *r* is the number of replications.

The function `covcor_design()` returns a list with the matrices of (co)variances and correlations. Specific matrices may be returned using the argument `type`, as shown bellow.

## Genetic correlations
```{r, fig.height = 5, fig.width = 5.5, fig.align = "center" }
data = subset(data_ge2, ENV == "A1")
covcor_design(data, gen = GEN, rep = REP,
              resp = c(PH, EH, NKE, TKW),
              type = "gcor")
    ```


## Phenotypic correlations
```{r, fig.height = 5, fig.width = 5.5, fig.align = "center" }
covcor_design(data, gen = GEN, rep = REP,
              resp = c(PH, EH, NKE, TKW),
              type = "pcor")
    ```

## Residual correlations
```{r, fig.height = 5, fig.width = 5.5, fig.align = "center" }
covcor_design(data, gen = GEN, rep = REP,
              resp = c(PH, EH, NKE, TKW),
              type = "rcor")
```

## Residual (co)variance matrix
Using the function `split_factors()` it is possible to pass the data grouped and compute the matrices for each level of the grouping factor. In this example we will obtain the residual (co)variance for each environment.

```{r, fig.height = 5, fig.width = 5.5, fig.align = "center", message=FALSE, warning=FALSE }
cov = data_ge2 %>%
      split_factors(ENV, keep_factors = TRUE) %>%
      covcor_design(GEN, REP, c(PH, EH, NKE, TKW),
                    type = "rcov")
    ```

The residual (co)variance matrix and the means (obtained using `type = "means"`) may be used into the function `mahala()` to compute the Mahalanobis distance 
```{r, fig.height = 5, fig.width = 5.5, fig.align = "center", message=FALSE, warning=FALSE }
res = data %>% # environment A1
      covcor_design(GEN, REP, c(PH, EH, NKE, TKW), type = "rcov")
means = data %>% # environment A1
      covcor_design(GEN, REP, c(PH, EH, NKE, TKW), type = "means")

D2 = mahala(.means = means, covar = res, inverted = FALSE)
    ```



## Nonparametric confidence interval for Pearson's correlation

Recently, a Gaussian-independent estimator for the confidence interval for Pearson's correlation coefficient was based proposed by @Olivoto2018. This estimator is based on sample size and strength of associations and may be estimated using the function `corr.ci()`. It is possible to estimate the confidence interval by declaring the sample size (n) and the correlation coefficient (), or using a dataframe.

```{r }
corr_ci(n = 145, r = 0.34)

corr_ci(data_ge2)

data_ge2 %>% split_factors(ENV) %>% corr_ci()
```


## Sample size planning

```{r }
corr_ss(r = 0.6, CI = 0.1)
    ```


# Collinearity diagnostic

The following codes compute a complete collinearity diagnostic of a correlation matrix of predictor traits. Several indicators, such as Variance Inflaction Factor, Condition Number, and Matrix Determinant are considered [@Olivoto2017f; @Olivoto2017c] The diagnostic may be performed using: (i) correlation matrices; (ii) dataframes, or (iii) an object of class `group_factor`, which split a dataframe into subsets based on one or more grouping factors.

## Using a correlation matrix, which was estimated earlier

```{r }
cor_data = lpc1$linear.mat
n = nrow(dataset)
cold1 = colindiag(cor_data, n = n)

```

## Using a dataframe

```{r }
cold2 = colindiag(dataset)

```


## Perform the diagnostic for each level of the factor ENV

```{r }
cold3 =  dataset %>% 
         split_factors(ENV, verbose = FALSE) %>% 
         colindiag()
```



# Path analysis
## Using KW as response trait and all other ones as predictors.

```{r }
pcoeff = data_ge2 %>%
         path_coeff(resp = KW)
```


## Declaring the predictor traits

```{r }
pcoeff2 = data_ge2 %>%
  path_coeff(resp = KW,
             pred = c(PH, NKE, TKW),
             verbose = FALSE)
summary(pcoeff2)
```


## Selecting traits to be excluded from the analysis.

```{r }
pcoeff2 = data_ge2 %>%
  path_coeff(resp = KW,
             pred = c(PH, EH, NKE, TKW),
             exclude = TRUE,
             verbose = FALSE)
```


## Selecting a set of predictors with minimal multicollinearity
```{r }
pcoeff3 = data_ge2 %>%
  path_coeff(resp = KW,
             brutstep = TRUE,
             maxvif = 5)
```


## Compute the analysis for each level of environment
```{r }
pcoeff4 = data_ge2 %>%
  split_factors(ENV) %>%
  path_coeff(resp = KW,
             pred = c(PH, EH, NKE, TKW))
```


# Canonical correlation analysis

```{r }
cc1 = can_corr(data_ge2,
               FG = c(PH, EH, EP),
               SG = c(EL, ED, CL, CD, CW, KW, NR))

cc2 = can_corr(FG = data_ge2[, 4:6],
               SG = data_ge2[, 7:13],
               verbose = FALSE,
               collinearity = FALSE)
```








# Clustering analysis
## Using function `clustering()`

```{r }
# All rows and all numeric variables from data
d1 = clustering(data_ge2)

# Based on the mean for each genotype
d2 = clustering(data_ge2, means_by = GEN)

# Based on the mean of each genotype
# Variables NKR, TKW, and NKE 
d3 = clustering(data_ge2, NKR, TKW, NKE, means_by = GEN)

# Select variables for compute the distances
d4 = clustering(data_ge2, means_by = GEN, selvar = TRUE)

# Compute the distances with standardized data
# Define 4 clusters
d5 = clustering(data_ge2,
                means_by = GEN,
                scale = TRUE,
                nclust = 4)

# Compute the distances for each environment
# All rows of each environment and all numeric variables used
d6 = data_ge2 %>%
     split_factors(ENV) %>%
     clustering()

# Compute the distances for each environment
# Select the variables NKR, TKW, and NKE
# Use the mean for each genotype
d7 = data_ge2 %>%
  split_factors(ENV, keep_factors = TRUE) %>%
  clustering(NKR, TKW, NKE,
             means_by = GEN)

# Check the correlation between distance matrices
pairs_mantel(d7)

    ```


## Mahalanobis distance
### Based on designed experiments

```{r }
options(digits = 2)
mahala_design(data_ge,
              gen = GEN,
              rep = REP,
              resp = c(GY, HM))

# Compute one distance for each environment
maha_group = data_ge %>%
             split_factors(ENV, keep_factors = TRUE) %>%
             mahala_design(GEN, REP, c(GY, HM))

    ```

### Compute one distance for each environment

```{r }
maha_group = data_ge %>%
             split_factors(ENV, keep_factors = TRUE) %>%
             mahala_design(GEN, REP, c(GY, HM))

    ```

### If you have the matrices of means and covariances

```{r }
means = data_ge %>%
        select(-c(ENV, REP)) %>%
        group_by(GEN) %>%
        summarise_all(mean) %>%
        select(-GEN)

# Compute the covariance matrix
covmat = cov(means)

# Compute the distance
dist = mahala(means, covmat)

# Dendrogram
dend = as.dendrogram(hclust(as.dist(dist)))
plot(dend)

    ```



#References

